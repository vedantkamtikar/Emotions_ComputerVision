{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0811e32c-40c4-47a3-9c55-eab392c7f5a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder, ImageNet\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch.cuda.amp import GradScaler\n",
    "import random, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c9f2fd-503e-4a57-bc11-463f54edd43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can access the GPU.\n",
      "Number of GPUs available: 1\n",
      "Current GPU Name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can access the GPU.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using the CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ad652a-a711-491c-b253-d2d5c2d32f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(\n",
    "    root=\"images/train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageFolder(\n",
    "    root=\"images/validation\",\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1080c3f5-4ee5-430f-af8a-690669587bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(class_names)\n",
    "print(f\"Number of classes: {num_classes}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41749213-dc7e-4b47-974a-3dcb2abfff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28821 7066\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train_dataset.classes == val_dataset.classes\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c527a5e-a39b-47fd-9508-b1dd265291d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=weights)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "#freezing the feature extractor\n",
    "\n",
    "model.classifier[1] = nn.Linear(\n",
    "    model.last_channel,\n",
    "    num_classes \n",
    "    \n",
    "    #Linear(1280 â†’ 36)\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b05950ad-cd85-443e-987c-6daf3f8578d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.classifier.parameters(),\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a4541c8-d3eb-40f6-bcb7-8e43a988e165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] - Train Loss: 1.6483, Train Acc: 35.42% - Val Loss: 1.5586, Val Acc: 40.11%\n",
      "Epoch [2/25] - Train Loss: 1.5402, Train Acc: 40.50% - Val Loss: 1.5231, Val Acc: 41.88%\n",
      "Epoch [3/25] - Train Loss: 1.5091, Train Acc: 41.85% - Val Loss: 1.5123, Val Acc: 41.92%\n",
      "Epoch [4/25] - Train Loss: 1.4894, Train Acc: 42.57% - Val Loss: 1.4965, Val Acc: 42.41%\n",
      "Epoch [5/25] - Train Loss: 1.4760, Train Acc: 43.08% - Val Loss: 1.4946, Val Acc: 42.56%\n",
      "Epoch [6/25] - Train Loss: 1.4680, Train Acc: 43.46% - Val Loss: 1.4938, Val Acc: 42.60%\n",
      "Epoch [7/25] - Train Loss: 1.4655, Train Acc: 43.45% - Val Loss: 1.4820, Val Acc: 43.46%\n",
      "Epoch [8/25] - Train Loss: 1.4559, Train Acc: 44.10% - Val Loss: 1.4880, Val Acc: 42.43%\n",
      "Epoch [9/25] - Train Loss: 1.4524, Train Acc: 44.28% - Val Loss: 1.4806, Val Acc: 42.95%\n",
      "Epoch [10/25] - Train Loss: 1.4493, Train Acc: 44.27% - Val Loss: 1.4793, Val Acc: 43.52%\n",
      "Epoch [11/25] - Train Loss: 1.4465, Train Acc: 44.60% - Val Loss: 1.4847, Val Acc: 42.58%\n",
      "Epoch [12/25] - Train Loss: 1.4466, Train Acc: 44.43% - Val Loss: 1.4808, Val Acc: 43.63%\n",
      "Epoch [13/25] - Train Loss: 1.4426, Train Acc: 44.58% - Val Loss: 1.4854, Val Acc: 43.42%\n",
      "Epoch [14/25] - Train Loss: 1.4449, Train Acc: 44.51% - Val Loss: 1.4736, Val Acc: 43.07%\n",
      "Epoch [15/25] - Train Loss: 1.4401, Train Acc: 44.67% - Val Loss: 1.4762, Val Acc: 43.79%\n",
      "Epoch [16/25] - Train Loss: 1.4409, Train Acc: 44.51% - Val Loss: 1.4770, Val Acc: 43.05%\n",
      "Epoch [17/25] - Train Loss: 1.4378, Train Acc: 44.46% - Val Loss: 1.4792, Val Acc: 43.25%\n",
      "Epoch [18/25] - Train Loss: 1.4374, Train Acc: 44.79% - Val Loss: 1.4804, Val Acc: 43.02%\n",
      "Epoch [19/25] - Train Loss: 1.4382, Train Acc: 44.78% - Val Loss: 1.4764, Val Acc: 43.91%\n",
      "Epoch [20/25] - Train Loss: 1.4409, Train Acc: 44.67% - Val Loss: 1.4793, Val Acc: 43.16%\n",
      "Epoch [21/25] - Train Loss: 1.4394, Train Acc: 44.62% - Val Loss: 1.4772, Val Acc: 42.87%\n",
      "Epoch [22/25] - Train Loss: 1.4364, Train Acc: 44.61% - Val Loss: 1.4795, Val Acc: 43.02%\n",
      "Epoch [23/25] - Train Loss: 1.4385, Train Acc: 44.43% - Val Loss: 1.4815, Val Acc: 43.19%\n",
      "Epoch [24/25] - Train Loss: 1.4408, Train Acc: 44.32% - Val Loss: 1.4759, Val Acc: 43.73%\n",
      "Epoch [25/25] - Train Loss: 1.4391, Train Acc: 44.68% - Val Loss: 1.4823, Val Acc: 43.11%\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "        f\"- Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "        f\"- Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884df1f2-fe1b-491b-b99c-e567ca0b7fce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"mobilnet_emotion.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1ac31d-9284-43f3-aba6-9f806bac1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"class_names.json\", \"w\") as f:\n",
    "    json.dump(class_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d0480ad-f84e-4936-b015-555c20543044",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=None)  # no pretrained at inference\n",
    "\n",
    "model.classifier[1] = torch.nn.Linear(\n",
    "    model.last_channel,\n",
    "    len(class_names)\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"mobilnet_emotion.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad2bf30d-dce2-4541-bc38-56a664f7a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    if img is None or img.size == 0:\n",
    "        return None\n",
    "\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    # Convert to GRAYSCALE first (to match training)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale to 3 channels\n",
    "    img = np.stack([img, img, img], axis=-1)\n",
    "\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img = (img - mean) / std\n",
    "\n",
    "    img = np.transpose(img, (2, 0, 1))  # CHW\n",
    "    img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a91942-6072-4010-8373-37da5fc0bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "mp_face = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "with mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.6) as face_detection:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(rgb)\n",
    "\n",
    "        if results.detections:\n",
    "            detection = results.detections[0]\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "\n",
    "            xmin = int(bbox.xmin * w)\n",
    "            ymin = int(bbox.ymin * h)\n",
    "            bw = int(bbox.width * w)\n",
    "            bh = int(bbox.height * h)\n",
    "\n",
    "            xmax = xmin + bw\n",
    "            ymax = ymin + bh\n",
    "\n",
    "            # Make square and add margin\n",
    "            box_size = max(bw, bh)\n",
    "            cx = xmin + bw // 2\n",
    "            cy = ymin + bh // 2\n",
    "\n",
    "            scale = 1.3\n",
    "            half = int(box_size * scale / 2)\n",
    "\n",
    "            xmin = max(0, cx - half)\n",
    "            ymin = max(0, cy - half)\n",
    "            xmax = min(w, cx + half)\n",
    "            ymax = min(h, cy + half)\n",
    "\n",
    "            face_crop = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            if face_crop.size != 0:\n",
    "                face_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "                face_pil = Image.fromarray(face_rgb)\n",
    "\n",
    "                input_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    probs = torch.softmax(output, dim=1)\n",
    "                    conf, pred = torch.max(probs, dim=1)\n",
    "\n",
    "                label = class_names[pred.item()]\n",
    "                confidence = conf.item()\n",
    "\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"{label} ({confidence:.2f})\",\n",
    "                    (xmin, ymin - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    (0, 255, 0),\n",
    "                    2\n",
    "                )\n",
    "\n",
    "                cv2.imshow(\"FACE_CROP_DEBUG\", face_crop)\n",
    "\n",
    "        cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd703f-67fd-470d-8747-7d390c18d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d776900-4d34-4fb0-9ee9-180e04862528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[1].out_features == len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c6cb6-db6a-4a90-ab99-d250e4223d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.classes == val_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1df014-2f9a-4214-ab4f-eb3b03a315c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
